{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Filter",
      "provenance": [],
      "authorship_tag": "ABX9TyNnCLQ95aOhbGJ6bwB0r6SD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArthurCalvi/Classifieur-Bois/blob/master/Filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olMXdXPU-iaU"
      },
      "source": [
        "# WOOD CLASSIFIER - Filter\n",
        "\n",
        "In this notebook, I provide a code to filter out false positives of a dataset with the help of the neural network. The net is used as a binary classifier to remove false positives from the dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcMHEAnN_Lh_"
      },
      "source": [
        "**PART 0 : Google Drive Access**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEk-OMOZVc7Y",
        "outputId": "cd813497-4510-4244-df05-173a708162fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8hTmMLx-lIz"
      },
      "source": [
        "***First run:*** The GitHub folder containing the codes, models and image directories is copied to your google drive with the following path: Google Drive -> Project_google_colab -> Classification-Bois."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXQnlhrL-gNI",
        "outputId": "7d89a1e4-d5bc-487d-d648-07d6f912490c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%cd /content/drive/My Drive/Project_google_colab\n",
        "! git clone https://github.com/ArthurCalvi/Classifieur-Bois"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Project_google_colab\n",
            "fatal: destination path 'Classifieur-Bois' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-GCETkI-ryv"
      },
      "source": [
        "***Other executions***: If you want to update the GitHub folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m7QpD1B-tHU",
        "outputId": "34fb42b0-b2d3-4454-fa1e-bcca4395e01c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "%cd /content/drive/My Drive/Project_google_colab/Classifieur-Bois\n",
        "! git pull"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Project_google_colab/Classifieur-Bois\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 43 (delta 23), reused 12 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (43/43), done.\n",
            "From https://github.com/ArthurCalvi/Classifieur-Bois\n",
            "   b1452b9..1e718f6  master     -> origin/master\n",
            "Updating b1452b9..1e718f6\n",
            "Fast-forward\n",
            " Creation_of_your_dataset.ipynb                     | 435 \u001b[32m+++++++++++++++++++++\u001b[m\n",
            " IMAGES_filtered/readme.txt                         |   1 \u001b[32m+\u001b[m\n",
            " ...Wood_Classifier.ipynb => Image_prediction.ipynb |   6 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " ...sing_images.ipynb => Images_preprocessing.ipynb |   8 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " README.md                                          |   3 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 5 files changed, 445 insertions(+), 8 deletions(-)\n",
            " create mode 100644 Creation_of_your_dataset.ipynb\n",
            " create mode 100644 IMAGES_filtered/readme.txt\n",
            " rename Neural_Net_Wood_Classifier.ipynb => Image_prediction.ipynb (99%)\n",
            " rename Neural_net_Preprocessing_images.ipynb => Images_preprocessing.ipynb (96%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9-KrcYx_sMu"
      },
      "source": [
        "**PART 1 : Upload the images of your dataset**\n",
        "\n",
        "Please upload all the images in the IMAGES_raw folder on google drive. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE6-nyFk_-vY"
      },
      "source": [
        "**PART 2: Creation of the Dataset**\n",
        "\n",
        "In this part all the images of the *IMAGES_raw* folders are first preprocessed and saved into the *IMAGES_preprocessed* folder, then the neural network is used to filter out the false positives and save the images of defects in the */IMAGES_filtered* folder. \n",
        "\n",
        "*NB : WARNING! All the images stored in the IMAGES_preprocessed folder are filtered.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-nDdFmMAltZ",
        "outputId": "4bf0837e-39d1-4de1-f99b-6e7b1d63a130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "##PARAMETERS\n",
        "#image width in px\n",
        "desired_size = 256\n",
        "\n",
        "#Nbr of augmented images used for prediction\n",
        "nbr_images = 10\n",
        "\n",
        "##IMPORTS\n",
        "#Import of the PIL image management library\n",
        "from PIL import Image \n",
        "\n",
        "#Import of os and sys libraries to manipulate files\n",
        "import sys,os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "#Imports of github personal files\n",
        "from custom_functions_v1 import crop_generator, random_crop, colorize_v2\n",
        "\n",
        "# Keras API\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "##FUNCTIONS\n",
        "def prep_image(image):\n",
        "    323/5000\n",
        "    \"\"\"This function takes as argument an image of any size and aspect opened\n",
        "    according to the PIL Image.open method and transforms it into a square\n",
        "    image of side 256 pixels. The cropping is performed such that the smallest\n",
        "    side is  fixed at 256 pixels and the largest side is then cropped to obtain\n",
        "    a square format.\"\"\"\n",
        "\n",
        "    #dimensions recovery\n",
        "    width,height = image.size\n",
        "\n",
        "    #smallest side recovery\n",
        "    small_side = min(width,height)\n",
        "\n",
        "    #definition of the ratio between the old and the new image\n",
        "    ratio = desired_size / small_side\n",
        "\n",
        "    #scaling\n",
        "    image = image.resize((round(ratio*width), round(ratio*height)), Image.ANTIALIAS )\n",
        "\n",
        "    #dimensions recovery\n",
        "    width,height = image.size\n",
        "\n",
        "    #definition of the region to crop\n",
        "    crop_region = ( round(( width-desired_size)/2), 0 , round((width+desired_size)/2) , desired_size)\n",
        "\n",
        "    #cropping \n",
        "    image = image.crop(crop_region)\n",
        "\n",
        "    return image\n",
        "\n",
        "def prep_and_save(path):\n",
        "    \"\"\"This function browses the images in the IMAGES_raw folder and saves it\n",
        "    in the IMAGES_preprocessed folder\"\"\"\n",
        "\n",
        "    dirs = os.listdir(path)\n",
        "    i=0\n",
        "\n",
        "    for item in dirs:\n",
        "        \n",
        "        super_path = path+\"/\"+item\n",
        "        \n",
        "        if os.path.isfile(super_path):\n",
        "            \n",
        "            image = Image.open(super_path)\n",
        "            \n",
        "            filename_w_ext = os.path.basename(super_path)\n",
        "            filename, extension = os.path.splitext(filename_w_ext)\n",
        "\n",
        "            image = prep_image(image)\n",
        "\n",
        "            image.save(save+\"/\"+filename+\"_{}.jpg\".format(desired_size), 'JPEG')\n",
        "            os.remove(super_path)\n",
        "\n",
        "            i+=1\n",
        "\n",
        "    print('{} images have been prepared and have been saved to Google Drive : My Drive/Project_google_colab/Classification-Bois/Images_preprocessed'.format(i))\n",
        "\n",
        "\n",
        "def prediction(img,nbr_images):\n",
        "    \"\"\" \n",
        "    Perform the prediction of the image [img] from [nbr_images] augmented\n",
        "    images.\n",
        "    \n",
        "    INPUT :\n",
        "        -img : array loaded with load_img \n",
        "        -nbr_images : integer representing the number of augmented images used\n",
        "         for prediction \n",
        "        \n",
        "        \n",
        "    OUTPUT :\n",
        "        -prediction : final prediction averaged over several images\n",
        "    \"\"\"\n",
        "    \n",
        "    # convert to numpy array\n",
        "    data = img_to_array(img)\n",
        "    # expand dimension to one sample\n",
        "    samples = np.expand_dims(data, 0)\n",
        "    \n",
        "    # create data for the test\n",
        "    datagen = ImageDataGenerator( fill_mode='reflect', \n",
        "                                  samplewise_center=True,\n",
        "                                  samplewise_std_normalization=True,\n",
        "                                  horizontal_flip=True, vertical_flip=True, \n",
        "                                  rotation_range=10, brightness_range= [0.6,1.4], \n",
        "                                  preprocessing_function = colorize_v2, \n",
        "                                  zoom_range = [1.0,1.3])\n",
        "    \n",
        "    batch = datagen.flow(samples, batch_size=1)\n",
        "    \n",
        "    #add random cropped\n",
        "    prediction = []\n",
        "    \n",
        "    for i in range(nbr_images):\n",
        "        \n",
        "        img = batch.next()\n",
        "        img = random_crop(img[0].astype('float32'), (224,224))\n",
        "        img = np.expand_dims(img, 0)\n",
        "        prediction.append(model.predict(img))\n",
        "    \n",
        "        \n",
        "    prediction = sum(prediction)/nbr_images\n",
        "    prediction = np.array(prediction).tolist()[0][0]\n",
        "    \n",
        "    return prediction  \n",
        "\n",
        "\n",
        "\n",
        "##SCRIPT\n",
        "\n",
        "#Directory to retrieve raw images\n",
        "access = '/content/drive/My Drive/Project_google_colab/Classifieur-Bois/IMAGES_raw'\n",
        "\n",
        "#Directory to save preprocessed images\n",
        "save = '/content/drive/My Drive/Project_google_colab/Classifieur-Bois/IMAGES_preprocessed'\n",
        "\n",
        "#Preparation of the images\n",
        "prep_and_save(access)\n",
        "\n",
        "#Loading model\n",
        "model = load_model('/content/drive/My Drive/Project_google_colab/Classifieur-Bois/MODEL_CNN1_bs32_ep100_augTrue_t1593511641.h5')\n",
        "\n",
        "#Use of the neural net to classify and save images\n",
        "dirs = os.listdir(save)\n",
        "\n",
        "i=0\n",
        "d=0\n",
        "fp=0\n",
        "for item in dirs:\n",
        "    \n",
        "    if os.path.isfile(save+'/'+item):\n",
        "\n",
        "        img = load_img(save+'/'+item)\n",
        "        score = prediction(img, nbr_images)\n",
        "        #revoir proba\n",
        "        if score>0.5:\n",
        "            img.save('/content/drive/My Drive/Project_google_colab/Classifieur-Bois/IMAGES_filtered/'+item)\n",
        "            i+=1\n",
        "            d+=1\n",
        "        else : \n",
        "            i+=1\n",
        "            fp+=1\n",
        "\n",
        "print(\"-----------------------------------------------------\")  \n",
        "print(\"{0} false positives detected ({1:.2%} of the dataset)\".format(fp, fp/i))\n",
        "print(\"{0} images kept ({1:.2%} of the dataset)\".format(d, d/i))\n",
        "print(\"-----------------------------------------------------\") "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 images have been prepared and have been saved to Google Drive : My Drive/Project_google_colab/Classification-Bois/Images_preprocessed\n",
            "-----------------------------------------------------\n",
            "2 false positives detected (22.22% of the dataset)\n",
            "7 images kept (77.78% of the dataset)\n",
            "-----------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia931eBpDoFK"
      },
      "source": [
        "**PART 3 - Warning !**\n",
        "\n",
        "The false positives of the dataset have been filtred out by the neural network wich have an overall accuracy of 94%. **To avoid mistakes in the folder /IMAGES_filtered I recommand a manual check of this folder.**"
      ]
    }
  ]
}