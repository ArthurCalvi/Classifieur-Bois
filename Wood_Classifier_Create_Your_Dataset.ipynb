{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wood Classifier : Create Your Dataset",
      "provenance": [],
      "authorship_tag": "ABX9TyNRvQtxFjFuuFA6pZtIrvxL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArthurCalvi/Classifieur-Bois/blob/master/Wood_Classifier_Create_Your_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olMXdXPU-iaU"
      },
      "source": [
        "# WOOD CLASSIFIER : CREATE YOUR DATASET\n",
        "\n",
        "In this notebook, I provide a code to create a dataset with the help of the neural network. The net is used as a binary classifier to save images into the defect or clearwood folder. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcMHEAnN_Lh_"
      },
      "source": [
        "**PART 0 : Google Drive Access**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEk-OMOZVc7Y",
        "outputId": "7910f111-58af-4ce1-bf38-db2981697c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8hTmMLx-lIz"
      },
      "source": [
        "***First run:*** The GitHub folder containing the codes, models and image directories is copied to your google drive with the following path: Google Drive -> Project_google_colab -> Classification-Bois."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXQnlhrL-gNI",
        "outputId": "7d89a1e4-d5bc-487d-d648-07d6f912490c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%cd /content/drive/My Drive/Project_google_colab\n",
        "! git clone https://github.com/ArthurCalvi/Classifieur-Bois"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Project_google_colab\n",
            "fatal: destination path 'Classifieur-Bois' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-GCETkI-ryv"
      },
      "source": [
        "***Other executions***: If you want to update the GitHub folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m7QpD1B-tHU",
        "outputId": "c236b56d-8762-41c1-db13-cde27e445345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%cd /content/drive/My Drive/Project_google_colab/Classifieur-Bois\n",
        "! git pull"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Project_google_colab/Classifieur-Bois\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9-KrcYx_sMu"
      },
      "source": [
        "**PART 1 : Upload the images of your dataset**\n",
        "\n",
        "Please upload all the images in the IMAGES_raw folder on google drive. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE6-nyFk_-vY"
      },
      "source": [
        "**PART 2: Creation of the Dataset**\n",
        "\n",
        "In this part all the images of the *IMAGES_raw* folders are first preprocessed and saved into the *IMAGES_preprocessed* folder, then the neural network is used to classify and save the images in the *New_Dataset/defect/* directory or *New_Dataset/clearwood/* directory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-nDdFmMAltZ",
        "outputId": "b33a7697-6fe3-48b4-8395-b3635759a668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "##PARAMETERS\n",
        "#image width in px\n",
        "desired_size = 256\n",
        "\n",
        "#Nbr of augmented images used for prediction\n",
        "nbr_images = 10\n",
        "\n",
        "##IMPORTS\n",
        "#Import of the PIL image management library\n",
        "from PIL import Image \n",
        "\n",
        "#Import of os and sys libraries to manipulate files\n",
        "import sys,os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "#Imports of github personal files\n",
        "from custom_functions_v1 import crop_generator, random_crop, colorize_v2\n",
        "\n",
        "# Keras API\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "##FUNCTIONS\n",
        "def prep_image(image):\n",
        "    323/5000\n",
        "    \"\"\"This function takes as argument an image of any size and aspect opened\n",
        "    according to the PIL Image.open method and transforms it into a square\n",
        "    image of side 256 pixels. The cropping is performed such that the smallest\n",
        "    side is  fixed at 256 pixels and the largest side is then cropped to obtain\n",
        "    a square format.\"\"\"\n",
        "\n",
        "    #dimensions recovery\n",
        "    width,height = image.size\n",
        "\n",
        "    #smallest side recovery\n",
        "    small_side = min(width,height)\n",
        "\n",
        "    #definition of the ratio between the old and the new image\n",
        "    ratio = desired_size / small_side\n",
        "\n",
        "    #scaling\n",
        "    image = image.resize((round(ratio*width), round(ratio*height)), Image.ANTIALIAS )\n",
        "\n",
        "    #dimensions recovery\n",
        "    width,height = image.size\n",
        "\n",
        "    #definition of the region to crop\n",
        "    crop_region = ( round(( width-desired_size)/2), 0 , round((width+desired_size)/2) , desired_size)\n",
        "\n",
        "    #cropping \n",
        "    image = image.crop(crop_region)\n",
        "\n",
        "    return image\n",
        "\n",
        "def prep_and_save(path):\n",
        "    \"\"\"This function browses the images in the IMAGES_raw folder and saves it\n",
        "    in the IMAGES_preprocessed folder\"\"\"\n",
        "\n",
        "    dirs = os.listdir(path)\n",
        "    i=0\n",
        "\n",
        "    for item in dirs:\n",
        "        \n",
        "        super_path = path+\"/\"+item\n",
        "        \n",
        "        if os.path.isfile(super_path):\n",
        "            \n",
        "            image = Image.open(super_path)\n",
        "            \n",
        "            filename_w_ext = os.path.basename(super_path)\n",
        "            filename, extension = os.path.splitext(filename_w_ext)\n",
        "\n",
        "            image = prep_image(image)\n",
        "\n",
        "            image.save(save+\"/\"+filename+\"_{}.jpg\".format(desired_size), 'JPEG')\n",
        "            os.remove(super_path)\n",
        "\n",
        "            i+=1\n",
        "\n",
        "    print('{} images have been prepared and have been saved to Google Drive : My Drive/Project_google_colab/Classification-Bois/Images_preprocessed'.format(i))\n",
        "\n",
        "\n",
        "def prediction(img,nbr_images):\n",
        "    \"\"\" \n",
        "    Perform the prediction of the image [img] from [nbr_images] augmented\n",
        "    images.\n",
        "    \n",
        "    INPUT :\n",
        "        -img : array loaded with load_img \n",
        "        -nbr_images : integer representing the number of augmented images used\n",
        "         for prediction \n",
        "        \n",
        "        \n",
        "    OUTPUT :\n",
        "        -prediction : final prediction averaged over several images\n",
        "    \"\"\"\n",
        "    \n",
        "    # convert to numpy array\n",
        "    data = img_to_array(img)\n",
        "    # expand dimension to one sample\n",
        "    samples = np.expand_dims(data, 0)\n",
        "    \n",
        "    # create data for the test\n",
        "    datagen = ImageDataGenerator( fill_mode='reflect', \n",
        "                                  samplewise_center=True,\n",
        "                                  samplewise_std_normalization=True,\n",
        "                                  horizontal_flip=True, vertical_flip=True, \n",
        "                                  rotation_range=10, brightness_range= [0.6,1.4], \n",
        "                                  preprocessing_function = colorize_v2, \n",
        "                                  zoom_range = [1.0,1.3])\n",
        "    \n",
        "    batch = datagen.flow(samples, batch_size=1)\n",
        "    \n",
        "    #add random cropped\n",
        "    prediction = []\n",
        "    \n",
        "    for i in range(nbr_images):\n",
        "        \n",
        "        img = batch.next()\n",
        "        img = random_crop(img[0].astype('float32'), (224,224))\n",
        "        img = np.expand_dims(img, 0)\n",
        "        prediction.append(model.predict(img))\n",
        "    \n",
        "        \n",
        "    prediction = sum(prediction)/nbr_images\n",
        "    prediction = np.array(prediction).tolist()[0][0]\n",
        "    \n",
        "    return prediction  \n",
        "\n",
        "\n",
        "\n",
        "##SCRIPT\n",
        "\n",
        "#Directory to retrieve raw images\n",
        "access = '/content/drive/My Drive/Project_google_colab/Classifieur-Bois/IMAGES_raw'\n",
        "\n",
        "#Directory to save preprocessed images\n",
        "save = '/content/drive/My Drive/Project_google_colab/Classifieur-Bois/IMAGES_preprocessed'\n",
        "\n",
        "#Preparation of the images\n",
        "prep_and_save(access)\n",
        "\n",
        "#Loading model\n",
        "model = load_model('/content/drive/My Drive/Project_google_colab/Classifieur-Bois/MODEL_CNN1_bs32_ep100_augTrue_t1593511641.h5')\n",
        "print(model.summary())\n",
        "\n",
        "#Use of the neural net to classify and save images\n",
        "dirs = os.listdir(save)\n",
        "\n",
        "i=0\n",
        "for item in dirs:\n",
        "    \n",
        "    if os.path.isfile(save+'/'+item):\n",
        "\n",
        "        img = load_img(save+'/'+item)\n",
        "        score = prediction(img, nbr_images)\n",
        "        print(item)\n",
        "        #revoir proba\n",
        "        if score>0.5:\n",
        "            img.save('/content/drive/My Drive/Project_google_colab/Classifieur-Bois/New_Dataset/defect/'+item)\n",
        "            i+=1\n",
        "        else : \n",
        "            img.save('/content/drive/My Drive/Project_google_colab/Classifieur-Bois/New_Dataset/clearwood/'+item)\n",
        "            i+=1\n",
        "  \n",
        "print(\"{} have been added into the New_Dataset\".format(i))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 images have been prepared and have been saved to Google Drive : My Drive/Project_google_colab/Classification-Bois/Images_preprocessed\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 222, 222, 32)      896       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 222, 222, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 111, 111, 32)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 111, 111, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 109, 109, 32)      9248      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 109, 109, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 54, 54, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 54, 54, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 52, 52, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 52, 52, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 26, 26, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 26, 26, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                589888    \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 628,737\n",
            "Trainable params: 628,289\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "None\n",
            "Wood-Wallpaper-Background-39_256.jpg\n",
            "clearwood_256_221.jpg\n",
            "clearwood_256_230.jpg\n",
            "defects_256_207.jpg\n",
            "th_256.jpg\n",
            "Burl_256.jpg\n",
            "wood-knot-stock-photo-101754_256.jpg\n",
            "wood_knot_texture_by_fotophi_256.jpg\n",
            "Red-Oak-Flooring_256.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia931eBpDoFK"
      },
      "source": [
        "**PART 3 - Warning !**\n",
        "\n",
        "The images of the dataset have been classified by the neural network wich have an overall accuracy of 94%. **To avoid mistakes in the dataset I recommand a manual check of the both folders : defect and clearwood.**"
      ]
    }
  ]
}